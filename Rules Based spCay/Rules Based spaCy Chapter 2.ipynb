{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "operational-landing",
   "metadata": {},
   "source": [
    "# <center>How to use the spaCy Matcher</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-understanding",
   "metadata": {},
   "source": [
    "<center>Dr. W.J.B. Mattingly</center>\n",
    "\n",
    "<center>Smithsonian Data Science Lab and United States Holocaust Memorial Museum</center>\n",
    "\n",
    "<center>August 2021</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "naked-lighting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-mechanism",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "described-clarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-delicious",
   "metadata": {},
   "source": [
    "## Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "technical-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LIKE_EMAIL\": True}]\n",
    "matcher.add(\"EMAIL_ADDRESS\", [pattern])\n",
    "doc = nlp(\"This is an email address: wmattingly@aol.com\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "synthetic-craps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16571425990740197027, 6, 7)]\n"
     ]
    }
   ],
   "source": [
    "print (matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-belarus",
   "metadata": {},
   "source": [
    "Lexeme, start token, end token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mounted-morning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMAIL_ADDRESS\n"
     ]
    }
   ],
   "source": [
    "print (nlp.vocab[matches[0][0]].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-nitrogen",
   "metadata": {},
   "source": [
    "## Attributes Taken by Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-democracy",
   "metadata": {},
   "source": [
    "* ORTH - The exact verbatim of a token (str)\n",
    "* TEXT - The exact verbatim of a token (str)\n",
    "* LOWER - The lowercase form of the token text (str)\n",
    "* LENGTH - The length of the token text (int)\n",
    "* IS_ALPHA\n",
    "* IS_ASCII\n",
    "* IS_DIGIT\n",
    "* IS_LOWER\n",
    "* IS_UPPER\n",
    "* IS_TITLE\n",
    "* IS_PUNCT\n",
    "* IS_SPACE\n",
    "* IS_STOP\n",
    "* IS_SENT_START\n",
    "* LIKE_NUM\n",
    "* LIKE_URL\n",
    "* LIKE_EMAIL\n",
    "* SPACY\n",
    "* POS\n",
    "* TAG\n",
    "* MORPH\n",
    "* DEP\n",
    "* LEMMA\n",
    "* SHAPE\n",
    "* ENT_TYPE\n",
    "* _ - Custom extension attributes (Dict\\[str, Any\\])\n",
    "* OP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-apache",
   "metadata": {},
   "source": [
    "## Applied Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fifth-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"/home/meghal/Personal/Konverge_AI/Training/NLP with SaCy/Data/wiki_mlk.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-automation",
   "metadata": {},
   "source": [
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "minute-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-vintage",
   "metadata": {},
   "source": [
    "## Grabbing all Proper Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cosmetic-combination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "(3232560085755078826, 0, 1) Martin\n",
      "(3232560085755078826, 1, 2) Luther\n",
      "(3232560085755078826, 2, 3) King\n",
      "(3232560085755078826, 3, 4) Jr.\n",
      "(3232560085755078826, 6, 7) Michael\n",
      "(3232560085755078826, 7, 8) King\n",
      "(3232560085755078826, 8, 9) Jr.\n",
      "(3232560085755078826, 10, 11) January\n",
      "(3232560085755078826, 15, 16) April\n",
      "(3232560085755078826, 23, 24) Baptist\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern])\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-hughes",
   "metadata": {},
   "source": [
    "### Improving it with Multi-Word Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "medical-method",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193\n",
      "(3232560085755078826, 0, 1) Martin\n",
      "(3232560085755078826, 0, 2) Martin Luther\n",
      "(3232560085755078826, 1, 2) Luther\n",
      "(3232560085755078826, 0, 3) Martin Luther King\n",
      "(3232560085755078826, 1, 3) Luther King\n",
      "(3232560085755078826, 2, 3) King\n",
      "(3232560085755078826, 0, 4) Martin Luther King Jr.\n",
      "(3232560085755078826, 1, 4) Luther King Jr.\n",
      "(3232560085755078826, 2, 4) King Jr.\n",
      "(3232560085755078826, 3, 4) Jr.\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern])\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-express",
   "metadata": {},
   "source": [
    "### Greedy Keyword Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "located-employer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "(3232560085755078826, 63, 68) Martin Luther King Sr.\n",
      "(3232560085755078826, 509, 514) Martin Luther King Jr. Day\n",
      "(3232560085755078826, 576, 581) Martin Luther King Jr. Memorial\n",
      "(3232560085755078826, 0, 4) Martin Luther King Jr.\n",
      "(3232560085755078826, 154, 158) Southern Christian Leadership Conference\n",
      "(3232560085755078826, 311, 315) Director J. Edgar Hoover\n",
      "(3232560085755078826, 6, 9) Michael King Jr.\n",
      "(3232560085755078826, 235, 238) Civil Rights Act\n",
      "(3232560085755078826, 241, 244) Voting Rights Act\n",
      "(3232560085755078826, 249, 252) Fair Housing Act\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-fitting",
   "metadata": {},
   "source": [
    "### Sorting it to Apperance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "blessed-begin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "(3232560085755078826, 0, 4) Martin Luther King Jr.\n",
      "(3232560085755078826, 6, 9) Michael King Jr.\n",
      "(3232560085755078826, 10, 11) January\n",
      "(3232560085755078826, 15, 16) April\n",
      "(3232560085755078826, 23, 24) Baptist\n",
      "(3232560085755078826, 38, 39) rights\n",
      "(3232560085755078826, 63, 68) Martin Luther King Sr.\n",
      "(3232560085755078826, 69, 70) King\n",
      "(3232560085755078826, 79, 81) United States\n",
      "(3232560085755078826, 97, 99) Mahatma Gandhi\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: x[1])\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-stupid",
   "metadata": {},
   "source": [
    "### Adding in Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "individual-timber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(3232560085755078826, 69, 71) King advanced\n",
      "(3232560085755078826, 117, 119) King participated\n",
      "(3232560085755078826, 311, 316) Director J. Edgar Hoover considered\n",
      "(3232560085755078826, 384, 386) King won\n",
      "(3232560085755078826, 525, 528) United States beginning\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}, {\"POS\": \"VERB\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: x[1])\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-stylus",
   "metadata": {},
   "source": [
    "## Finding Quotes and Speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "similar-budget",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/alice.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-783473e17d1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"data/alice.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/alice.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open (\"data/alice.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-nutrition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, `and what is the use of a book,' thought Alice `without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "text = data[0][2][0]\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-ghost",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "text = data[0][2][0].replace( \"`\", \"'\")\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-integration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(3232560085755078826, 47, 58) 'and what is the use of a book,'\n",
      "(3232560085755078826, 60, 67) 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: x[1])\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-emperor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-pastor",
   "metadata": {},
   "source": [
    "### Find Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-heath",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(3232560085755078826, 47, 67) 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "speak_lemmas = [\"think\", \"say\"]\n",
    "text = data[0][2][0].replace( \"`\", \"'\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}, {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {\"POS\": \"PROPN\", \"OP\": \"+\"}, {'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern1], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: x[1])\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-immigration",
   "metadata": {},
   "source": [
    "### Problem with this Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-payday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(3232560085755078826, 47, 67) 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for text in data[0][2]:\n",
    "    text = text.replace(\"`\", \"'\")\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    matches.sort(key = lambda x: x[1])\n",
    "    print (len(matches))\n",
    "    for match in matches[:10]:\n",
    "        print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-dynamics",
   "metadata": {},
   "source": [
    "### Adding More Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-locking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(3232560085755078826, 47, 67) 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "(3232560085755078826, 0, 6) 'Well!' thought Alice\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "(3232560085755078826, 57, 68) 'which certainly was not here before,' said Alice\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "speak_lemmas = [\"think\", \"say\"]\n",
    "text = data[0][2][0].replace( \"`\", \"'\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}, {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {\"POS\": \"PROPN\", \"OP\": \"+\"}, {'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}]\n",
    "pattern2 = [{'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}, {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
    "pattern3 = [{\"POS\": \"PROPN\", \"OP\": \"+\"},{\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern1, pattern2, pattern3], greedy='LONGEST')\n",
    "for text in data[0][2]:\n",
    "    text = text.replace(\"`\", \"'\")\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    matches.sort(key = lambda x: x[1])\n",
    "    print (len(matches))\n",
    "    for match in matches[:10]:\n",
    "        print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-auditor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
